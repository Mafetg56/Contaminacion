# -*- coding: utf-8 -*-
"""Copia de Copia de Trabajo final MFTG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UovsyjgWsEojujazVR-hawfNJl-DNbXz
"""

ls



import pandas as pd
df=pd.read_excel("Mediciones_Calidad_Aire_La_Candelaria_Organizado.xlsx")

print(df)



df.head()

df.info()

df = df.drop(['Fecha y Hora de Inicio (dd/MM/aaaa  HH:mm:ss)', 'Fecha y Hora de Finalización (dd/MM/aaaa  HH:mm:ss)', 'Precipitación (mm)'], axis=1)
print(df.head())

#  HTML para descargar con ydata profiling

!pip install ydata-profiling
from ydata_profiling import ProfileReport

# Assuming 'df' is your pandas DataFrame from the previous code
profile = ProfileReport(df, title="Mediciones_Calidad_Aire_La_Candelaria_Organizado Profiling Report", minimal=True)

profile.to_file("mediciones_calidad_aire_profile.html")

print("Profile report generated: mediciones_calidad_aire_profile.html")

# prompt: Usa mutual information para encontrar variables redundantes, crea un HTML descargable para visualizar la información

from sklearn.feature_selection import mutual_info_regression
import numpy as np
import pandas as pd
from google.colab import files
import base64

# Calculate mutual information between all pairs of numerical columns
df_numeric = df.select_dtypes(include=np.number)
mi_matrix = pd.DataFrame(np.zeros((df_numeric.shape[1], df_numeric.shape[1])),
                         columns=df_numeric.columns, index=df_numeric.columns)

for col1 in df_numeric.columns:
    for col2 in df_numeric.columns:
        if col1 != col2:
            mi_matrix.loc[col1, col2] = mutual_info_regression(df_numeric[[col1]], df_numeric[col2])[0]

# Create an HTML report
html_content = """
<!DOCTYPE html>
<html>
<head>
<title>Mutual Information Report</title>
<style>
    table {{
        border-collapse: collapse;
        width: 100%;
    }}
    th, td {{
        border: 1px solid #ddd;
        padding: 8px;
        text-align: right;
    }}
    th {{
        background-color: #f2f2f2;
    }}
    .heatmap {{
        background-color: rgba(255, 0, 0, {{alpha}});
    }}
</style>
</head>
<body>

<h1>Mutual Information Matrix</h1>

<p>This table shows the mutual information score between each pair of numerical variables. Higher values indicate stronger dependencies.</p>

<table>
  <thead>
    <tr>
      <th></th>
      {}
    </tr>
  </thead>
  <tbody>
    {}
  </tbody>
</table>

</body>
</html>
""".format(
    ''.join(f'<th>{col}</th>' for col in mi_matrix.columns),
    ''.join(f"""
    <tr>
        <th>{index}</th>
        {''.join(f'<td class="heatmap" style="background-color: rgba(255, 0, 0, {value/mi_matrix.values.max()});">{value:.4f}</td>' for value in row)}
    </tr>""" for index, row in mi_matrix.iterrows())
)

# Encode HTML content to base64
html_bytes = html_content.encode('utf-8')
encoded_html = base64.b64encode(html_bytes).decode('utf-8')

# Create a downloadable link
download_link = f"""
<a href="data:text/html;charset=utf-8;base64,{encoded_html}" download="mutual_information_report.html">Download Mutual Information Report</a>
"""

from IPython.display import display, HTML
display(HTML(download_link))

df = df.drop(['Humedad Relativa 10m (%)'], axis=1)
print(df.head())

# prompt: construya diagrama de cajas y bigotes para las variable numéricas y muestre los registros de atípicos

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Select only numerical columns for the box plot
df_numeric = df.select_dtypes(include=np.number)

# Create box plots for each numerical variable
plt.figure(figsize=(15, 10))
sns.boxplot(data=df_numeric)
plt.title('Box Plots of Numerical Variables')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Identify and display outliers
outliers = {}
for col in df_numeric.columns:
    Q1 = df_numeric[col].quantile(0.25)
    Q3 = df_numeric[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    col_outliers = df_numeric[(df_numeric[col] < lower_bound) | (df_numeric[col] > upper_bound)]
    if not col_outliers.empty:
        outliers[col] = col_outliers

print("\nOutlier Records:")
for col, outlier_df in outliers.items():
    print(f"\nOutliers for {col}:")
outlier_df

# prompt: Usa standar scaler y escala todas las variables excepto PM10 (ug/m3)\nCondición Estándar

import numpy as np
from sklearn.preprocessing import StandardScaler

# Create a copy of the DataFrame to avoid modifying the original
df_scaled = df.copy()

# Identify columns to scale (all numerical except 'PM10 (ug/m3) Condición Estándar')
columns_to_scale = df_scaled.select_dtypes(include=np.number).columns.tolist()
if 'PM10 (ug/m3) Condición Estándar' in columns_to_scale:
    columns_to_scale.remove('PM10 (ug/m3) Condición Estándar')

# Initialize the StandardScaler
scaler = StandardScaler()

# Apply the scaler to the selected columns
df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])

print(df_scaled.head())

# prompt: realiza análisis de componentes principales PCA en un HTML

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Standardize the data before applying PCA
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_numeric)

# Apply PCA
# You can choose the number of components to keep. For now, let's keep all components to analyze variance.
pca = PCA()
principal_components = pca.fit_transform(df_scaled)

# Create a DataFrame with the principal components
pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])

# Explainable variance ratio
explained_variance = pca.explained_variance_ratio_

# Cumulative explained variance
cumulative_explained_variance = np.cumsum(explained_variance)

# Create HTML for PCA results
html_content_pca = """
<!DOCTYPE html>
<html>
<head>
<title>PCA Report</title>
<style>
    table {{
        border-collapse: collapse;
        width: 100%;
    }}
    th, td {{
        border: 1px solid #ddd;
        padding: 8px;
        text-align: right;
    }}
    th {{
        background-color: #f2f2f2;
    }}
</style>
</head>
<body>

<h1>Principal Component Analysis (PCA) Report</h1>

<h2>Explained Variance Ratio</h2>
<p>This table shows the proportion of variance explained by each principal component.</p>
<table>
  <thead>
    <tr>
      <th>Principal Component</th>
      <th>Explained Variance Ratio</th>
      <th>Cumulative Explained Variance</th>
    </tr>
  </thead>
  <tbody>
    {}
  </tbody>
</table>

<h2>Principal Components DataFrame</h2>
<p>This is the DataFrame with the data transformed by PCA.</p>
{}

</body>
</html>
""".format(
    ''.join(f"""
    <tr>
        <td>PC{i+1}</td>
        <td>{explained_variance[i]:.4f}</td>
        <td>{cumulative_explained_variance[i]:.4f}</td>
    </tr>""" for i in range(len(explained_variance))),
    pca_df.to_html()
)

# Encode HTML content to base64
html_bytes_pca = html_content_pca.encode('utf-8')
encoded_html_pca = base64.b64encode(html_bytes_pca).decode('utf-8')

# Create a downloadable link for the PCA report
download_link_pca = f"""
<a href="data:text/html;charset=utf-8;base64,{encoded_html_pca}" download="pca_report.html">Download PCA Report</a>
"""

display(HTML(download_link_pca))

# Optional: Plot the explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')
plt.title('Explained Variance by Number of Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(range(1, len(explained_variance) + 1), explained_variance)
plt.title('Explained Variance per Principal Component')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.show()

# Optional: Visualize the first two principal components (if applicable and meaningful)
if principal_components.shape[1] >= 2:
    plt.figure(figsize=(10, 8))
    plt.scatter(principal_components[:, 0], principal_components[:, 1])
    plt.title('PCA - First Two Principal Components')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.grid(True)
    plt.show()

# prompt: Construya una regresión lineal usando división 70-30, usando scikitlearn, la variable objetivo es PM10 (ug/m3)
# Condición Estándar mida el MAE, el MAPE, le RMSE y R2 del modelo

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd # Import pandas

# Assuming 'df' is your pandas DataFrame from the previous code after dropping columns
# The target variable is 'PM10 (ug/m3)\nCondición Estándar'

# Print the column names to verify the exact name
print("DataFrame columns:", df.columns)

# Correct the target variable name by replacing the newline character
target_variable = 'PM10 (ug/m3)\nCondición Estándar'

# Define features (X) and target (y)
# We will use all numerical columns except the target variable as features
numerical_cols = df.select_dtypes(include=np.number).columns
feature_cols = [col for col in numerical_cols if col != target_variable]

X = df[feature_cols]
y = df[target_variable]

# Split the data into training and testing sets (70-30 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

# Calculate MAPE
# Avoid division by zero by adding a small epsilon or filtering out zero values if necessary
# For simplicity, we'll calculate MAPE only for non-zero actual values
y_test_non_zero = y_test[y_test != 0]
y_pred_non_zero = y_pred[y_test != 0]
mape = np.mean(np.abs((y_test_non_zero - y_pred_non_zero) / y_test_non_zero)) * 100 if len(y_test_non_zero) > 0 else float('inf')


print("Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R2): {r2:.4f}")

# Optional: Display the coefficients and intercept of the model
print("\nModel Coefficients:")
for feature, coef in zip(feature_cols, model.coef_):
    print(f"{feature}: {coef:.4f}")
print(f"Intercept: {model.intercept_:.4f}")

# prompt: Construya un KNN usando scikitlearn, la variable objetivo es PM10 (ug/m3)
# Condición Estándar, mide los las metricas de evaluación del modelo

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Create the KNN Regressor model
# You can adjust the number of neighbors (n_neighbors) as a hyperparameter
knn_model = KNeighborsRegressor(n_neighbors=5)

# Train the KNN model
knn_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_knn = knn_model.predict(X_test)

# Evaluate the KNN model
mae_knn = mean_absolute_error(y_test, y_pred_knn)
rmse_knn = np.sqrt(mean_squared_error(y_test, y_pred_knn))
r2_knn = r2_score(y_test, y_pred_knn)

# Calculate MAPE for KNN
y_test_non_zero_knn = y_test[y_test != 0]
y_pred_non_zero_knn = y_pred_knn[y_test != 0]
mape_knn = np.mean(np.abs((y_test_non_zero_knn - y_pred_non_zero_knn) / y_test_non_zero_knn)) * 100 if len(y_test_non_zero_knn) > 0 else float('inf')


print("\nKNN Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae_knn:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_knn:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_knn:.4f}")
print(f"R-squared (R2): {r2_knn:.4f}")

# prompt: Construya una máquina de soporte vectorial usando scikitlearn, la variable objetivo es PM10 (ug/m3), mide los las metricas de evaluación del modelo

import numpy as np
from sklearn.svm import SVR

# Create the Support Vector Regressor model
# You can choose different kernels (linear, poly, rbf, sigmoid) and adjust other parameters
# For a start, let's use the default RBF kernel
svm_model = SVR()

# Train the SVR model
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_svm = svm_model.predict(X_test)

# Evaluate the SVR model
mae_svm = mean_absolute_error(y_test, y_pred_svm)
rmse_svm = np.sqrt(mean_squared_error(y_test, y_pred_svm))
r2_svm = r2_score(y_test, y_pred_svm)

# Calculate MAPE for SVR
y_test_non_zero_svm = y_test[y_test != 0]
y_pred_non_zero_svm = y_pred_svm[y_test != 0]
mape_svm = np.mean(np.abs((y_test_non_zero_svm - y_pred_non_zero_svm) / y_test_non_zero_svm)) * 100 if len(y_test_non_zero_svm) > 0 else float('inf')

print("\nSupport Vector Regressor (SVR) Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae_svm:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_svm:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_svm:.4f}")
print(f"R-squared (R2): {r2_svm:.4f}")

# prompt: Construya una Regresión Lasso usando scikitlearn, la variable objetivo es PM10 (ug/m3), mide los las metricas de evaluación del modelo

import numpy as np
from sklearn.linear_model import Lasso

# Create the Lasso Regression model
# You can adjust the regularization strength using the 'alpha' parameter.
# A higher alpha value means stronger regularization (more coefficients pushed to zero).
# Let's start with a common value, like alpha=1.0
lasso_model = Lasso(alpha=1.0)

# Train the Lasso model
lasso_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_lasso = lasso_model.predict(X_test)

# Evaluate the Lasso model
mae_lasso = mean_absolute_error(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))
r2_lasso = r2_score(y_test, y_pred_lasso)

# Calculate MAPE for Lasso
y_test_non_zero_lasso = y_test[y_test != 0]
y_pred_non_zero_lasso = y_pred_lasso[y_test != 0]
mape_lasso = np.mean(np.abs((y_test_non_zero_lasso - y_pred_non_zero_lasso) / y_test_non_zero_lasso)) * 100 if len(y_test_non_zero_lasso) > 0 else float('inf')

print("\nLasso Regression Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae_lasso:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_lasso:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_lasso:.4f}")
print(f"R-squared (R2): {r2_lasso:.4f}")

# Optional: Display the coefficients and intercept of the Lasso model
print("\nLasso Model Coefficients:")
for feature, coef in zip(feature_cols, lasso_model.coef_):
    print(f"{feature}: {coef:.4f}")
print(f"Intercept: {lasso_model.intercept_:.4f}")

# prompt: Construya un arbol de decisión usando scikitlearn, la variable objetivo es PM10 (ug/m3), mide los las metricas de evaluación del modelo

import numpy as np
from sklearn.tree import DecisionTreeRegressor

# Create the Decision Tree Regressor model
# You can adjust hyperparameters like max_depth, min_samples_split, etc.
dt_model = DecisionTreeRegressor(random_state=42) # Using random_state for reproducibility

# Train the Decision Tree model
dt_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_dt = dt_model.predict(X_test)

# Evaluate the Decision Tree model
mae_dt = mean_absolute_error(y_test, y_pred_dt)
rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))
r2_dt = r2_score(y_test, y_pred_dt)

# Calculate MAPE for Decision Tree
y_test_non_zero_dt = y_test[y_test != 0]
y_pred_non_zero_dt = y_pred_dt[y_test != 0]
mape_dt = np.mean(np.abs((y_test_non_zero_dt - y_pred_non_zero_dt) / y_test_non_zero_dt)) * 100 if len(y_test_non_zero_dt) > 0 else float('inf')

print("\nDecision Tree Regressor Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae_dt:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_dt:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_dt:.4f}")
print(f"R-squared (R2): {r2_dt:.4f}")

# prompt: Construya un Voting Regressor usando scikitlearn, la variable objetivo es PM10 (ug/m3), mide los las metricas de evaluación del modelo

from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Define the individual regressors
reg1 = LinearRegression()
reg2 = KNeighborsRegressor(n_neighbors=5)
reg3 = SVR()
reg4 = Lasso(alpha=1.0)
reg5 = DecisionTreeRegressor(random_state=42)

# Create the Voting Regressor
# You can adjust the 'weights' parameter to give different importance to each model
# You can also try different 'voting' strategies ('hard' or 'soft' for classification, not applicable directly here but conceptually similar for averaging)
voting_regressor = VotingRegressor(estimators=[('lr', reg1), ('knn', reg2), ('svm', reg3), ('lasso', reg4), ('dt', reg5)])

# Train the Voting Regressor
voting_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred_voting = voting_regressor.predict(X_test)

# Evaluate the Voting Regressor model
mae_voting = mean_absolute_error(y_test, y_pred_voting)
rmse_voting = np.sqrt(mean_squared_error(y_test, y_pred_voting))
r2_voting = r2_score(y_test, y_pred_voting)

# Calculate MAPE for Voting Regressor
y_test_non_zero_voting = y_test[y_test != 0]
y_pred_non_zero_voting = y_pred_voting[y_test != 0]
mape_voting = np.mean(np.abs((y_test_non_zero_voting - y_pred_non_zero_voting) / y_test_non_zero_voting)) * 100 if len(y_test_non_zero_voting) > 0 else float('inf')

print("\nVoting Regressor Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae_voting:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_voting:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_voting:.4f}")
print(f"R-squared (R2): {r2_voting:.4f}")

# prompt: Construya un Random forest regressor usando scikitlearn, la variable objetivo es PM10 (ug/m3), mide los las metricas de evaluación del modelo

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestRegressor

# Create the Random Forest Regressor model
# You can adjust hyperparameters like n_estimators (number of trees), max_depth, etc.
# Using random_state for reproducibility
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the Random Forest model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the Random Forest model
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

# Calculate MAPE for Random Forest
y_test_non_zero_rf = y_test[y_test != 0]
y_pred_non_zero_rf = y_pred_rf[y_test != 0]
mape_rf = np.mean(np.abs((y_test_non_zero_rf - y_pred_non_zero_rf) / y_test_non_zero_rf)) * 100 if len(y_test_non_zero_rf) > 0 else float('inf')

print("\nRandom Forest Regressor Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae_rf:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_rf:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_rf:.4f}")
print(f"R-squared (R2): {r2_rf:.4f}")

# Optional: Display feature importances
print("\nRandom Forest Feature Importances:")
feature_importances = pd.Series(rf_model.feature_importances_, index=feature_cols).sort_values(ascending=False)
print(feature_importances)

plt.figure(figsize=(10, 6))
feature_importances.plot(kind='bar')
plt.title('Random Forest Feature Importances')
plt.ylabel('Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# prompt: Construya un Gradient Boosting Regressor usando scikitlearn, la variable objetivo es PM10 (ug/m3), mide los las metricas de evaluación del modelo

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np # Make sure numpy is imported if not already

# Create the Gradient Boosting Regressor model
# You can adjust hyperparameters like n_estimators, learning_rate, max_depth, etc.
# Using random_state for reproducibility
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Train the Gradient Boosting model
gbr_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_gbr = gbr_model.predict(X_test)

# Evaluate the Gradient Boosting model
mae_gbr = mean_absolute_error(y_test, y_pred_gbr)
rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))
r2_gbr = r2_score(y_test, y_pred_gbr)

# Calculate MAPE for Gradient Boosting
y_test_non_zero_gbr = y_test[y_test != 0]
y_pred_non_zero_gbr = y_pred_gbr[y_test != 0]
mape_gbr = np.mean(np.abs((y_test_non_zero_gbr - y_pred_non_zero_gbr) / y_test_non_zero_gbr)) * 100 if len(y_test_non_zero_gbr) > 0 else float('inf')

print("\nGradient Boosting Regressor Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae_gbr:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_gbr:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_gbr:.4f}")
print(f"R-squared (R2): {r2_gbr:.4f}")

# Optional: Display feature importances (Gradient Boosting models also provide this)
print("\nGradient Boosting Feature Importances:")
feature_importances_gbr = pd.Series(gbr_model.feature_importances_, index=feature_cols).sort_values(ascending=False)
print(feature_importances_gbr)

plt.figure(figsize=(10, 6))
feature_importances_gbr.plot(kind='bar')
plt.title('Gradient Boosting Feature Importances')
plt.ylabel('Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# prompt: guarda los modelos y los escaladores

import joblib

# Save the models
joblib.dump(model, 'linear_regression_model.pkl')
joblib.dump(knn_model, 'knn_model.pkl')
joblib.dump(svm_model, 'svm_model.pkl')
joblib.dump(lasso_model, 'lasso_model.pkl')
joblib.dump(dt_model, 'decision_tree_model.pkl')
joblib.dump(voting_regressor, 'voting_regressor_model.pkl')
joblib.dump(rf_model, 'random_forest_model.pkl')
joblib.dump(gbr_model, 'gradient_boosting_model.pkl')

# Save the scaler used for PCA (if you want to reuse it)
# Note: The scaler used for standardizing features before training models is likely different
# from the one used before PCA, depending on the columns included.
# You might need to initialize and fit a new scaler on X_train if you want to save that one.
# For simplicity, we'll save the scaler used in the initial scaling step (`df_scaled`).
# Assuming you want to save the scaler that was fitted on 'columns_to_scale'.
# Re-initialize and fit the scaler on the training data features to ensure consistency for future use.
scaler_for_features = StandardScaler()
scaler_for_features.fit(X_train) # Fit on the training features
joblib.dump(scaler_for_features, 'feature_scaler.pkl')

print("Models and scaler saved successfully.")

# prompt: Para la regresión lineal usa GridSearchCV para buscar la mejor combinación de hiperparámetros con el conjunto de entrenamiento y utiliza validación cruzada internamente que minimice RMSE. Especifica la métrica (scoring='neg_root_mean_squared_error'

import numpy as np
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Linear Regression (no hyperparameters to tune in standard Linear Regression)
# We can still use GridSearchCV to evaluate performance with cross-validation.
# However, since there are no hyperparameters, the grid search will just run the model
# with the default settings multiple times across the cross-validation folds.
# If you had regularized versions like Ridge or Lasso, you would tune alpha here.
param_grid_lr = {}

# Create a Linear Regression model instance
linear_regression_model = LinearRegression()

# Set up GridSearchCV
# scoring='neg_root_mean_squared_error' aims to minimize RMSE by maximizing the negative RMSE.
# cv specifies the number of folds for cross-validation.
grid_search_lr = GridSearchCV(
    estimator=linear_regression_model,
    param_grid=param_grid_lr,
    scoring='neg_root_mean_squared_error',
    cv=5,  # Using 5-fold cross-validation
    n_jobs=-1 # Use all available cores
)

# Perform the grid search on the training data
grid_search_lr.fit(X_train, y_train)

# Get the best parameters and best score
best_params_lr = grid_search_lr.best_params_
best_score_lr = grid_search_lr.best_score_ # This is the mean cross-validated negative RMSE

# Convert the best score to RMSE (since it's negative RMSE)
best_rmse_lr = -best_score_lr

print("\nGrid Search Results for Linear Regression:")
print(f"Best Parameters: {best_params_lr}")
print(f"Best Cross-Validated RMSE: {best_rmse_lr:.4f}")

# The best estimator is the model trained on the entire training set using the best parameters found (though for LR, there are none).
best_linear_regression_model = grid_search_lr.best_estimator_

# Evaluate the best model on the test set
y_pred_best_lr = best_linear_regression_model.predict(X_test)

# Calculate evaluation metrics for the best model on the test set
mae_best_lr = mean_absolute_error(y_test, y_pred_best_lr)
rmse_best_lr = np.sqrt(mean_squared_error(y_test, y_pred_best_lr))
r2_best_lr = r2_score(y_test, y_pred_best_lr)

# Calculate MAPE for the best Linear Regression model
y_test_non_zero_best_lr = y_test[y_test != 0]
y_pred_non_zero_best_lr = y_pred_best_lr[y_test != 0]
mape_best_lr = np.mean(np.abs((y_test_non_zero_best_lr - y_pred_non_zero_best_lr) / y_test_non_zero_best_lr)) * 100 if len(y_test_non_zero_best_lr) > 0 else float('inf')


print("\nEvaluation Metrics for Best Linear Regression Model (on Test Set):")
print(f"Mean Absolute Error (MAE): {mae_best_lr:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_best_lr:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_best_lr:.4f}")
print(f"R-squared (R2): {r2_best_lr:.4f}")

# Optional: Display the coefficients and intercept of the best model
print("\nBest Linear Regression Model Coefficients:")
for feature, coef in zip(feature_cols, best_linear_regression_model.coef_):
    print(f"{feature}: {coef:.4f}")
print(f"Intercept: {best_linear_regression_model.intercept_:.4f}")

# Save the best Linear Regression model
joblib.dump(best_linear_regression_model, 'best_linear_regression_model.pkl')
print("\nBest Linear Regression model saved as best_linear_regression_model.pkl")

# prompt: Para knn usa GridSearchCV para buscar la mejor combinación de hiperparámetros con el conjunto de entrenamiento y utiliza validación cruzada internamente que minimice RMSE. Especifica la métrica (scoring='neg_root_mean_squared_error'

import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsRegressor

# Define the parameter grid for KNN
param_grid_knn = {
    'n_neighbors': range(1, 31),  # Explore number of neighbors from 1 to 30
    'weights': ['uniform', 'distance'], # Explore uniform and distance weighting
    'metric': ['euclidean', 'manhattan'] # Explore different distance metrics
}

# Create a KNN Regressor model instance
knn_model_gs = KNeighborsRegressor()

# Set up GridSearchCV for KNN
grid_search_knn = GridSearchCV(
    estimator=knn_model_gs,
    param_grid=param_grid_knn,
    scoring='neg_root_mean_squared_error', # Use negative RMSE to minimize RMSE
    cv=5,  # Using 5-fold cross-validation
    n_jobs=-1 # Use all available cores
)

# Perform the grid search on the training data
grid_search_knn.fit(X_train, y_train)

# Get the best parameters and best score for KNN
best_params_knn = grid_search_knn.best_params_
best_score_knn = grid_search_knn.best_score_ # This is the mean cross-validated negative RMSE

# Convert the best score to RMSE (since it's negative RMSE)
best_rmse_knn = -best_score_knn

print("\nGrid Search Results for KNN Regressor:")
print(f"Best Parameters: {best_params_knn}")
print(f"Best Cross-Validated RMSE: {best_rmse_knn:.4f}")

# The best estimator is the model trained on the entire training set using the best parameters found.
best_knn_model = grid_search_knn.best_estimator_

# Evaluate the best KNN model on the test set
y_pred_best_knn = best_knn_model.predict(X_test)

# Calculate evaluation metrics for the best KNN model on the test set
mae_best_knn = mean_absolute_error(y_test, y_pred_best_knn)
rmse_best_knn = np.sqrt(mean_squared_error(y_test, y_pred_best_knn))
r2_best_knn = r2_score(y_test, y_pred_best_knn)

# Calculate MAPE for the best KNN model
y_test_non_zero_best_knn = y_test[y_test != 0]
y_pred_non_zero_best_knn = y_pred_best_knn[y_test != 0]
mape_best_knn = np.mean(np.abs((y_test_non_zero_best_knn - y_pred_non_zero_best_knn) / y_test_non_zero_best_knn)) * 100 if len(y_test_non_zero_best_knn) > 0 else float('inf')

print("\nEvaluation Metrics for Best KNN Regressor Model (on Test Set):")
print(f"Mean Absolute Error (MAE): {mae_best_knn:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_best_knn:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_best_knn:.4f}")
print(f"R-squared (R2): {r2_best_knn:.4f}")

# Save the best KNN model
joblib.dump(best_knn_model, 'best_knn_model.pkl')
print("\nBest KNN Regressor model saved as best_knn_model.pkl")

# prompt: Para Maquina de soporte vectorial usa GridSearchCV para buscar la mejor combinación de hiperparámetros con el conjunto de entrenamiento y utiliza validación cruzada internamente que minimice RMSE.

import numpy as np
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for SVR
# These are common hyperparameters for SVR
param_grid_svm = {
    'kernel': ['rbf'],  # RBF kernel is often a good starting point
    'C': [0.1, 1, 10, 100],  # Regularization parameter
    'epsilon': [0.01, 0.1, 0.5] # Epsilon in the epsilon-SVR model
    # You could also explore 'gamma': ['scale', 'auto'] or specific values
    # For polynomial kernel: 'degree', 'coef0'
}

# Create an SVR model instance
svm_model_gs = SVR()

# Set up GridSearchCV for SVR
grid_search_svm = GridSearchCV(
    estimator=svm_model_gs,
    param_grid=param_grid_svm,
    scoring='neg_root_mean_squared_error', # Use negative RMSE to minimize RMSE
    cv=5,  # Using 5-fold cross-validation
    n_jobs=-1 # Use all available cores
)

# Perform the grid search on the training data
print("\nStarting Grid Search for SVR...")
grid_search_svm.fit(X_train, y_train)
print("Grid Search for SVR finished.")

# Get the best parameters and best score for SVR
best_params_svm = grid_search_svm.best_params_
best_score_svm = grid_search_svm.best_score_ # This is the mean cross-validated negative RMSE

# Convert the best score to RMSE (since it's negative RMSE)
best_rmse_svm = -best_score_svm

print("\nGrid Search Results for SVR Regressor:")
print(f"Best Parameters: {best_params_svm}")
print(f"Best Cross-Validated RMSE: {best_rmse_svm:.4f}")

# The best estimator is the model trained on the entire training set using the best parameters found.
best_svm_model = grid_search_svm.best_estimator_

# Evaluate the best SVR model on the test set
y_pred_best_svm = best_svm_model.predict(X_test)

# Calculate evaluation metrics for the best SVR model on the test set
mae_best_svm = mean_absolute_error(y_test, y_pred_best_svm)
rmse_best_svm = np.sqrt(mean_squared_error(y_test, y_pred_best_svm))
r2_best_svm = r2_score(y_test, y_pred_best_svm)

# Calculate MAPE for the best SVR model
y_test_non_zero_best_svm = y_test[y_test != 0]
y_pred_non_zero_best_svm = y_pred_best_svm[y_test != 0]
mape_best_svm = np.mean(np.abs((y_test_non_zero_best_svm - y_pred_non_zero_best_svm) / y_test_non_zero_best_svm)) * 100 if len(y_test_non_zero_best_svm) > 0 else float('inf')

print("\nEvaluation Metrics for Best SVR Regressor Model (on Test Set):")
print(f"Mean Absolute Error (MAE): {mae_best_svm:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_best_svm:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_best_svm:.4f}")
print(f"R-squared (R2): {r2_best_svm:.4f}")

# Save the best SVR model
joblib.dump(best_svm_model, 'best_svm_model.pkl')
print("\nBest SVR Regressor model saved as best_svm_model.pkl")

# prompt: Para Regresión Lasso usa GridSearchCV para buscar la mejor combinación de hiperparámetros con el conjunto de entrenamiento y utiliza validación cruzada internamente que minimice RMSE. Especifica la métrica (scoring='neg_root_mean_squared_error'

import numpy as np
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Lasso Regression
param_grid_lasso = {
    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] # Values for the regularization strength
}

# Create a Lasso Regression model instance
lasso_model_gs = Lasso(random_state=42) # Add random_state for reproducibility

# Set up GridSearchCV for Lasso
grid_search_lasso = GridSearchCV(
    estimator=lasso_model_gs,
    param_grid=param_grid_lasso,
    scoring='neg_root_mean_squared_error', # Use negative RMSE to minimize RMSE
    cv=5,  # Using 5-fold cross-validation
    n_jobs=-1 # Use all available cores
)

# Perform the grid search on the training data
print("\nStarting Grid Search for Lasso...")
grid_search_lasso.fit(X_train, y_train)
print("Grid Search for Lasso finished.")

# Get the best parameters and best score for Lasso
best_params_lasso = grid_search_lasso.best_params_
best_score_lasso = grid_search_lasso.best_score_ # This is the mean cross-validated negative RMSE

# Convert the best score to RMSE (since it's negative RMSE)
best_rmse_lasso = -best_score_lasso

print("\nGrid Search Results for Lasso Regressor:")
print(f"Best Parameters: {best_params_lasso}")
print(f"Best Cross-Validated RMSE: {best_rmse_lasso:.4f}")

# The best estimator is the model trained on the entire training set using the best parameters found.
best_lasso_model = grid_search_lasso.best_estimator_

# Evaluate the best Lasso model on the test set
y_pred_best_lasso = best_lasso_model.predict(X_test)

# Calculate evaluation metrics for the best Lasso model on the test set
mae_best_lasso = mean_absolute_error(y_test, y_pred_best_lasso)
rmse_best_lasso = np.sqrt(mean_squared_error(y_test, y_pred_best_lasso))
r2_best_lasso = r2_score(y_test, y_pred_best_lasso)

# Calculate MAPE for the best Lasso model
y_test_non_zero_best_lasso = y_test[y_test != 0]
y_pred_non_zero_best_lasso = y_pred_best_lasso[y_test != 0]
mape_best_lasso = np.mean(np.abs((y_test_non_zero_best_lasso - y_pred_non_zero_best_lasso) / y_test_non_zero_best_lasso)) * 100 if len(y_test_non_zero_best_lasso) > 0 else float('inf')

print("\nEvaluation Metrics for Best Lasso Regressor Model (on Test Set):")
print(f"Mean Absolute Error (MAE): {mae_best_lasso:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_best_lasso:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_best_lasso:.4f}")
print(f"R-squared (R2): {r2_best_lasso:.4f}")

# Save the best Lasso model
joblib.dump(best_lasso_model, 'best_lasso_model.pkl')
print("\nBest Lasso Regressor model saved as best_lasso_model.pkl")

# prompt: Para Arbol de decisión usa GridSearchCV para buscar la mejor combinación de hiperparámetros con el conjunto de entrenamiento y utiliza validación cruzada internamente que minimice RMSE. Especifica la métrica (scoring='neg_root_mean_squared_error'

import numpy as np
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Decision Tree Regressor
# These are common hyperparameters to tune for Decision Trees
param_grid_dt = {
    'max_depth': [None, 10, 20, 30, 40, 50], # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],        # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]           # Minimum number of samples required to be at a leaf node
}

# Create a Decision Tree Regressor model instance
dt_model_gs = DecisionTreeRegressor(random_state=42) # Use random_state for reproducibility

# Set up GridSearchCV for Decision Tree Regressor
grid_search_dt = GridSearchCV(
    estimator=dt_model_gs,
    param_grid=param_grid_dt,
    scoring='neg_root_mean_squared_error', # Use negative RMSE to minimize RMSE
    cv=5,  # Using 5-fold cross-validation
    n_jobs=-1 # Use all available cores
)

# Perform the grid search on the training data
print("\nStarting Grid Search for Decision Tree Regressor...")
grid_search_dt.fit(X_train, y_train)
print("Grid Search for Decision Tree Regressor finished.")

# Get the best parameters and best score for Decision Tree Regressor
best_params_dt = grid_search_dt.best_params_
best_score_dt = grid_search_dt.best_score_ # This is the mean cross-validated negative RMSE

# Convert the best score to RMSE (since it's negative RMSE)
best_rmse_dt = -best_score_dt

print("\nGrid Search Results for Decision Tree Regressor:")
print(f"Best Parameters: {best_params_dt}")
print(f"Best Cross-Validated RMSE: {best_rmse_dt:.4f}")

# The best estimator is the model trained on the entire training set using the best parameters found.
best_dt_model = grid_search_dt.best_estimator_

# Evaluate the best Decision Tree model on the test set
y_pred_best_dt = best_dt_model.predict(X_test)

# Calculate evaluation metrics for the best Decision Tree model on the test set
mae_best_dt = mean_absolute_error(y_test, y_pred_best_dt)
rmse_best_dt = np.sqrt(mean_squared_error(y_test, y_pred_best_dt))
r2_best_dt = r2_score(y_test, y_pred_best_dt)

# Calculate MAPE for the best Decision Tree model
y_test_non_zero_best_dt = y_test[y_test != 0]
y_pred_non_zero_best_dt = y_pred_best_dt[y_test != 0]
mape_best_dt = np.mean(np.abs((y_test_non_zero_best_dt - y_pred_non_zero_best_dt) / y_test_non_zero_best_dt)) * 100 if len(y_test_non_zero_best_dt) > 0 else float('inf')

print("\nEvaluation Metrics for Best Decision Tree Regressor Model (on Test Set):")
print(f"Mean Absolute Error (MAE): {mae_best_dt:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_best_dt:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_best_dt:.4f}")
print(f"R-squared (R2): {r2_best_dt:.4f}")

# Save the best Decision Tree model
joblib.dump(best_dt_model, 'best_decision_tree_model.pkl')
print("\nBest Decision Tree Regressor model saved as best_decision_tree_model.pkl")

# prompt: Para Voting Regressor usa GridSearchCV para buscar la mejor combinación de hiperparámetros con el conjunto de entrenamiento y utiliza validación cruzada internamente que minimice RMSE.

import numpy as np
from sklearn.model_selection import GridSearchCV

# Re-define the individual regressors (can use the best found models from previous grid searches if available,
# but for demonstrating hyperparameter tuning on the VotingRegressor itself,
# it's common to use the base models without their individual tuning first, or use the *best* individual models found).
# Let's use the *best* models found from the previous grid searches.
reg1_best = best_linear_regression_model # From LR GridSearch
reg2_best = best_knn_model              # From KNN GridSearch
reg3_best = best_svm_model              # From SVR GridSearch
reg4_best = best_lasso_model            # From Lasso GridSearch
reg5_best = best_dt_model               # From Decision Tree GridSearch

# Create the Voting Regressor instance using the best individual models
voting_regressor_gs = VotingRegressor(
    estimators=[
        ('lr', reg1_best),
        ('knn', reg2_best),
        ('svm', reg3_best),
        ('lasso', reg4_best),
        ('dt', reg5_best)
    ]
    # You can also add 'n_jobs' here if needed
)

# Define the parameter grid for the Voting Regressor
# We can tune the weights of the individual regressors.
# The weights should sum to 1.
# Example: [[w1, w2, w3, w4, w5]] where w1+w2+w3+w4+w5 = 1
# This can be computationally expensive depending on the number of regressors and grid points.
# A simpler approach is to try different combinations where some weights are equal or zero.
# For demonstration, let's try a few simple weight combinations.
# Alternatively, we could just use uniform weights and see how it performs.

# Let's define a grid for weights. This can become complex quickly.
# Example: Trying uniform weights, and giving more weight to one model at a time.
param_grid_voting = {
    'weights': [
        [1/5, 1/5, 1/5, 1/5, 1/5],  # Uniform weights
        [0.5, 0.125, 0.125, 0.125, 0.125], # More weight to LR
        [0.125, 0.5, 0.125, 0.125, 0.125], # More weight to KNN
        [0.125, 0.125, 0.5, 0.125, 0.125], # More weight to SVR
        [0.125, 0.125, 0.125, 0.5, 0.125], # More weight to Lasso
        [0.125, 0.125, 0.125, 0.125, 0.5]  # More weight to Decision Tree
        # Add more combinations as needed
    ]
}

# Set up GridSearchCV for the Voting Regressor
grid_search_voting = GridSearchCV(
    estimator=voting_regressor_gs,
    param_grid=param_grid_voting,
    scoring='neg_root_mean_squared_error', # Use negative RMSE to minimize RMSE
    cv=5,  # Using 5-fold cross-validation
    n_jobs=-1 # Use all available cores
)

# Perform the grid search on the training data
print("\nStarting Grid Search for Voting Regressor...")
grid_search_voting.fit(X_train, y_train)
print("Grid Search for Voting Regressor finished.")

# Get the best parameters and best score for the Voting Regressor
best_params_voting = grid_search_voting.best_params_
best_score_voting = grid_search_voting.best_score_ # This is the mean cross-validated negative RMSE

# Convert the best score to RMSE (since it's negative RMSE)
best_rmse_voting = -best_score_voting

print("\nGrid Search Results for Voting Regressor:")
print(f"Best Parameters: {best_params_voting}")
print(f"Best Cross-Validated RMSE: {best_rmse_voting:.4f}")

# The best estimator is the model trained on the entire training set using the best parameters found.
best_voting_regressor_model = grid_search_voting.best_estimator_

# Evaluate the best Voting Regressor model on the test set
y_pred_best_voting = best_voting_regressor_model.predict(X_test)

# Calculate evaluation metrics for the best Voting Regressor model on the test set
mae_best_voting = mean_absolute_error(y_test, y_pred_best_voting)
rmse_best_voting = np.sqrt(mean_squared_error(y_test, y_pred_best_voting))
r2_best_voting = r2_score(y_test, y_pred_best_voting)

# Calculate MAPE for the best Voting Regressor model
y_test_non_zero_best_voting = y_test[y_test != 0]
y_pred_non_zero_best_voting = y_pred_best_voting[y_test != 0]
mape_best_voting = np.mean(np.abs((y_test_non_zero_best_voting - y_pred_non_zero_best_voting) / y_test_non_zero_best_voting)) * 100 if len(y_test_non_zero_best_voting) > 0 else float('inf')

print("\nEvaluation Metrics for Best Voting Regressor Model (on Test Set):")
print(f"Mean Absolute Error (MAE): {mae_best_voting:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_best_voting:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_best_voting:.4f}")
print(f"R-squared (R2): {r2_best_voting:.4f}")

# Save the best Voting Regressor model
joblib.dump(best_voting_regressor_model, 'best_voting_regressor_model.pkl')
print("\nBest Voting Regressor model saved as best_voting_regressor_model.pkl")

# prompt: Para Random Forest Regressor usa GridSearchCV para buscar la mejor combinación de hiperparámetros con el conjunto de entrenamiento y utiliza validación cruzada internamente

import numpy as np
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Random Forest Regressor
# These are common hyperparameters to tune for Random Forests
param_grid_rf = {
    'n_estimators': [100, 200, 300],         # Number of trees in the forest
    'max_depth': [None, 10, 20],             # Maximum depth of the trees
    'min_samples_split': [2, 5],             # Minimum number of samples required to split a node
    'min_samples_leaf': [1, 2],              # Minimum number of samples required at a leaf node
    'max_features': ['sqrt', 'log2', None] # Number of features to consider when looking for the best split
}

# Create a Random Forest Regressor model instance
rf_model_gs = RandomForestRegressor(random_state=42) # Use random_state for reproducibility

# Set up GridSearchCV for Random Forest Regressor
grid_search_rf = GridSearchCV(
    estimator=rf_model_gs,
    param_grid=param_grid_rf,
    scoring='neg_root_mean_squared_error', # Use negative RMSE to minimize RMSE
    cv=5,  # Using 5-fold cross-validation
    n_jobs=-1 # Use all available cores
)

# Perform the grid search on the training data
print("\nStarting Grid Search for Random Forest Regressor...")
grid_search_rf.fit(X_train, y_train)
print("Grid Search for Random Forest Regressor finished.")

# Get the best parameters and best score for Random Forest Regressor
best_params_rf = grid_search_rf.best_params_
best_score_rf = grid_search_rf.best_score_ # This is the mean cross-validated negative RMSE

# Convert the best score to RMSE (since it's negative RMSE)
best_rmse_rf = -best_score_rf

print("\nGrid Search Results for Random Forest Regressor:")
print(f"Best Parameters: {best_params_rf}")
print(f"Best Cross-Validated RMSE: {best_rmse_rf:.4f}")

# The best estimator is the model trained on the entire training set using the best parameters found.
best_random_forest_model = grid_search_rf.best_estimator_

# Evaluate the best Random Forest model on the test set
y_pred_best_rf = best_random_forest_model.predict(X_test)

# Calculate evaluation metrics for the best Random Forest model on the test set
mae_best_rf = mean_absolute_error(y_test, y_pred_best_rf)
rmse_best_rf = np.sqrt(mean_squared_error(y_test, y_pred_best_rf))
r2_best_rf = r2_score(y_test, y_pred_best_rf)

# Calculate MAPE for the best Random Forest model
y_test_non_zero_best_rf = y_test[y_test != 0]
y_pred_non_zero_best_rf = y_pred_best_rf[y_test != 0]
mape_best_rf = np.mean(np.abs((y_test_non_zero_best_rf - y_pred_non_zero_best_rf) / y_test_non_zero_best_rf)) * 100 if len(y_test_non_zero_best_rf) > 0 else float('inf')

print("\nEvaluation Metrics for Best Random Forest Regressor Model (on Test Set):")
print(f"Mean Absolute Error (MAE): {mae_best_rf:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_best_rf:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_best_rf:.4f}")
print(f"R-squared (R2): {r2_best_rf:.4f}")

# Save the best Random Forest model
joblib.dump(best_random_forest_model, 'best_random_forest_model.pkl')
print("\nBest Random Forest Regressor model saved as best_random_forest_model.pkl")

# prompt: Para Gradient Boosting Regressor usa GridSearchCV para buscar la mejor combinación de hiperparámetros con el conjunto de entrenamiento y utiliza validación cruzada internamente

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Gradient Boosting Regressor
# These are common hyperparameters to tune for Gradient Boosting
param_grid_gbr = {
    'n_estimators': [100, 200, 300],        # Number of boosting stages
    'learning_rate': [0.01, 0.1, 0.2],      # Shrinkage factor applied to each tree
    'max_depth': [3, 4, 5],                 # Maximum depth of the individual regression estimators
    'min_samples_split': [2, 5],            # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2]              # Minimum number of samples required to be at a leaf node
}

# Create a Gradient Boosting Regressor model instance
gbr_model_gs = GradientBoostingRegressor(random_state=42) # Use random_state for reproducibility

# Set up GridSearchCV for Gradient Boosting Regressor
# scoring='neg_root_mean_squared_error' aims to minimize RMSE by maximizing the negative RMSE.
# cv specifies the number of folds for cross-validation.
grid_search_gbr = GridSearchCV(
    estimator=gbr_model_gs,
    param_grid=param_grid_gbr,
    scoring='neg_root_mean_squared_error',
    cv=5,  # Using 5-fold cross-validation
    n_jobs=-1 # Use all available cores
)

# Perform the grid search on the training data
print("\nStarting Grid Search for Gradient Boosting Regressor...")
grid_search_gbr.fit(X_train, y_train)
print("Grid Search for Gradient Boosting Regressor finished.")

# Get the best parameters and best score
best_params_gbr = grid_search_gbr.best_params_
best_score_gbr = grid_search_gbr.best_score_ # This is the mean cross-validated negative RMSE

# Convert the best score to RMSE (since it's negative RMSE)
best_rmse_gbr = -best_score_gbr

print("\nGrid Search Results for Gradient Boosting Regressor:")
print(f"Best Parameters: {best_params_gbr}")
print(f"Best Cross-Validated RMSE: {best_rmse_gbr:.4f}")

# The best estimator is the model trained on the entire training set using the best parameters found.
best_gradient_boosting_model = grid_search_gbr.best_estimator_

# Evaluate the best model on the test set
y_pred_best_gbr = best_gradient_boosting_model.predict(X_test)

# Calculate evaluation metrics for the best Gradient Boosting model on the test set
mae_best_gbr = mean_absolute_error(y_test, y_pred_best_gbr)
rmse_best_gbr = np.sqrt(mean_squared_error(y_test, y_pred_best_gbr))
r2_best_gbr = r2_score(y_test, y_pred_best_gbr)

# Calculate MAPE for the best Gradient Boosting model
y_test_non_zero_best_gbr = y_test[y_test != 0]
y_pred_non_zero_best_gbr = y_pred_best_gbr[y_test != 0]
mape_best_gbr = np.mean(np.abs((y_test_non_zero_best_gbr - y_pred_non_zero_best_gbr) / y_test_non_zero_best_gbr)) * 100 if len(y_test_non_zero_best_gbr) > 0 else float('inf')

print("\nEvaluation Metrics for Best Gradient Boosting Regressor Model (on Test Set):")
print(f"Mean Absolute Error (MAE): {mae_best_gbr:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_best_gbr:.4f}%")
print(f"Root Mean Squared Error (RMSE): {rmse_best_gbr:.4f}")
print(f"R-squared (R2): {r2_best_gbr:.4f}")

# Optional: Display feature importances for the best model
print("\nBest Gradient Boosting Model Feature Importances:")
feature_importances_best_gbr = pd.Series(best_gradient_boosting_model.feature_importances_, index=feature_cols).sort_values(ascending=False)
print(feature_importances_best_gbr)

plt.figure(figsize=(10, 6))
feature_importances_best_gbr.plot(kind='bar')
plt.title('Best Gradient Boosting Feature Importances')
plt.ylabel('Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Save the best Gradient Boosting model
joblib.dump(best_gradient_boosting_model, 'best_gradient_boosting_model.pkl')
print("\nBest Gradient Boosting Regressor model saved as best_gradient_boosting_model.pkl")

